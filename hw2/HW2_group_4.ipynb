{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80jDMGpra6GN"
      },
      "outputs": [],
      "source": [
        "#  Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "yF_IYFLKbHz0",
        "outputId": "432aa2d1-a4a2-4446-fba3-a9b70171c594"
      },
      "outputs": [],
      "source": [
        "# Make sure to upload \"credit_risk_dataset.csv\" to your Colab session.\n",
        "df = pd.read_csv(\"credit_risk_dataset.csv\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-48WJEV8bOfE",
        "outputId": "d2f22f03-d467-4013-ed80-e65b22565711"
      },
      "outputs": [],
      "source": [
        "# Identify numeric and categorical columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Fill missing numeric values with the median and categorical with the mode\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "print(\"Missing values after imputation:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "HFwqL_pSbhc0",
        "outputId": "d3f01d42-4980-4ffc-bd19-4fa5fafea798"
      },
      "outputs": [],
      "source": [
        "# Corrected feature list with proper column names as provided\n",
        "cluster_features = ['person_age', 'person_income', 'person_emp_length',\n",
        "                    'loan_amnt', 'loan_int_rate', 'loan_percent_income',\n",
        "                    'cb_person_cred_hist_length']\n",
        "print(\"Columns used for clustering:\", cluster_features)\n",
        "\n",
        "# Extract the selected features from the dataframe\n",
        "X_cluster = df[cluster_features].copy()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_cluster)\n",
        "\n",
        "# Determine the optimal number of clusters using the Elbow Method\n",
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K_range, inertia, 'bo-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method to Determine Optimal k')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC6cxJe7cRMU",
        "outputId": "c53cb629-0580-45f2-c4ca-38d60525ea5a"
      },
      "outputs": [],
      "source": [
        "# From the elbow plot, suppose we choose k = 3 (you may adjust based on the plot)\n",
        "optimal_k = 3\n",
        "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "cluster_labels = kmeans_model.fit_predict(X_scaled)\n",
        "\n",
        "# Add cluster labels to the dataframe and view the cluster distribution\n",
        "df['cluster'] = cluster_labels\n",
        "print(\"Cluster counts:\")\n",
        "print(df['cluster'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "7HXf_LUGcXO0",
        "outputId": "93f37164-8969-4573-cf69-a5a6e3d1b61e"
      },
      "outputs": [],
      "source": [
        "# Use PCA to reduce dimensions to 2 for visualization of clusters\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=cluster_labels, palette='viridis')\n",
        "plt.title('Loan Applicant Clusters (PCA Projection)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Summarize cluster characteristics (mean values for key loan features)\n",
        "cluster_summary = df.groupby('cluster')[['person_income', 'loan_amnt', 'loan_int_rate', 'loan_status']].mean()\n",
        "print(\"Cluster Summary (Mean Values):\")\n",
        "print(cluster_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcDwlYpmcsmZ",
        "outputId": "a857a7df-a9fd-4fd1-95b2-7a9ddcda252b"
      },
      "outputs": [],
      "source": [
        "print(\"Unique values in 'loan_intent':\", df['loan_intent'].unique())\n",
        "\n",
        "# Create dummy variables for 'loan_intent'\n",
        "df_encoded = pd.get_dummies(df, columns=['loan_intent'], drop_first=True)\n",
        "encoded_columns = [col for col in df_encoded.columns if col.startswith('loan_intent_')]\n",
        "print(\"Encoded loan_intent columns:\", encoded_columns)\n",
        "\n",
        "# Define X (features) and y (target) using only the loan_intent dummies for simplicity.\n",
        "X_class = df_encoded[encoded_columns]\n",
        "y_class = df_encoded['loan_status']\n",
        "\n",
        "# Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_class, y_class,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_class)\n",
        "\n",
        "# Train a logistic regression model with balanced class weights\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "print(\"\\nLogistic Regression Model Evaluation (with class weights):\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_mat)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "McFROZd3gNdO",
        "outputId": "bec29019-a5eb-441e-ac0c-aab7eb3abc90"
      },
      "outputs": [],
      "source": [
        "# Expanded Feature Logistic Regression with Grid Search for Hyperparameter Tuning. We wanted to see if we could gain more accuracy by using gridsearch to help with proper parameters. they were basic parameters though\n",
        "\n",
        "# 1. Select additional numerical features\n",
        "features = ['person_income', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_age']\n",
        "df_features = df[features].copy()\n",
        "\n",
        "# 2. One-hot encode the 'loan_intent' categorical variable\n",
        "df_intent = pd.get_dummies(df['loan_intent'], prefix='loan_intent', drop_first=True)\n",
        "\n",
        "# 3. Combine the numerical features with the one-hot encoded features\n",
        "X_full = pd.concat([df_features, df_intent], axis=1)\n",
        "y_full = df['loan_status']\n",
        "\n",
        "# 4. Standardize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_full[features] = scaler.fit_transform(X_full[features])\n",
        "\n",
        "# 5. Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.3, random_state=42, stratify=y_full\n",
        ")\n",
        "\n",
        "# 6. Hyperparameter tuning using GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # liblinear supports both l1 and l2\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "    param_grid, cv=5, scoring='precision', n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters from GridSearchCV:\", grid_search.best_params_)\n",
        "\n",
        "# 7. Evaluate the best model from grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_grid = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nGridSearchCV Best Model Evaluation:\")\n",
        "print(classification_report(y_test, y_pred_grid, zero_division=0))\n",
        "\n",
        "# Optional: Plotting the confusion matrix for visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_mat_grid = confusion_matrix(y_test, y_pred_grid)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(conf_mat_grid, annot=True, fmt=\"d\", cmap='Blues',\n",
        "            xticklabels=['Predicted Non-default','Predicted Default'],\n",
        "            yticklabels=['Actual Non-default','Actual Default'])\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.title(\"Confusion Matrix (Best GridSearchCV Model)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzTjC9XEfTuv",
        "outputId": "c46e255c-0685-44ab-ad25-c91a3c611686"
      },
      "outputs": [],
      "source": [
        "# Expanded Feature Set for Improved Classification\n",
        "\n",
        "# 1. Select additional numerical features\n",
        "features = ['person_income', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_age']\n",
        "df_features = df[features].copy()\n",
        "\n",
        "# 2. One-hot encode the 'loan_intent' categorical variable\n",
        "df_intent = pd.get_dummies(df['loan_intent'], prefix='loan_intent', drop_first=True)\n",
        "\n",
        "# 3. Combine the numerical features with the one-hot encoded features\n",
        "X_full = pd.concat([df_features, df_intent], axis=1)\n",
        "y_full = df['loan_status']\n",
        "\n",
        "# 4. Standardize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_full[features] = scaler.fit_transform(X_full[features])\n",
        "\n",
        "# 5. Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.3, random_state=42, stratify=y_full\n",
        ")\n",
        "\n",
        "# 6. Train a logistic regression model with balanced class weights\n",
        "logreg_full = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "logreg_full.fit(X_train, y_train)\n",
        "\n",
        "# 7. Predict on the test set\n",
        "y_pred_full = logreg_full.predict(X_test)\n",
        "\n",
        "# 8. Evaluate model performance\n",
        "conf_mat_full = confusion_matrix(y_test, y_pred_full)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "precision_full = precision_score(y_test, y_pred_full, zero_division=0)\n",
        "recall_full = recall_score(y_test, y_pred_full, zero_division=0)\n",
        "f1_full = f1_score(y_test, y_pred_full, zero_division=0)\n",
        "\n",
        "print(\"Expanded Feature Logistic Regression Model Evaluation:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_mat_full)\n",
        "print(f\"Accuracy: {accuracy_full:.4f}\")\n",
        "print(f\"Precision: {precision_full:.4f}\")\n",
        "print(f\"Recall: {recall_full:.4f}\")\n",
        "print(f\"F1 Score: {f1_full:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_full, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWLbh7CAUA8D"
      },
      "source": [
        "Questions:\n",
        "\n",
        "1. We rate this class a 5\n",
        "\n",
        "2. for difficulty we rate this class a 3\n",
        "\n",
        "3. A lot of the material we are learning is all applicable to our careers, but it allows us to learn new methods and other forms of applied areas in finance. The machine learning and regression components have been really helpful, because some of us are going into the quant field.\n",
        "\n",
        "4. The challenges we would say is the learning curve on the math side. We can code out the logic, but would like to be better on the math side if models aren't correct.\n",
        "\n",
        "5. We think your doing a great job. The only suggestion would be to recommend more github accounts to look at or research papers for application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2OTK0eMUDlD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
