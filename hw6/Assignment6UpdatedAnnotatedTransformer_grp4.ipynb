{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6eef4c",
   "metadata": {},
   "source": [
    "# Updated Annotated Transformer\n",
    "\n",
    "This notebook is the fully updated version of the Harvard Annotated Transformer. All code cells have been modernized to work with recent PyTorch releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73360201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e47bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"Scaled Dot-Product Attention\"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1784ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"Take in model size and number of heads.\"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k  \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch  \n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear  \n",
    "        x = x.transpose(1, 2).contiguous()              .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14cd4f",
   "metadata": {},
   "source": [
    "## Survey Responses\n",
    "\n",
    "**Guest Speakers Ratings (avg):**\n",
    "- Nadia: 8/10\n",
    "- Jeff: 9/10\n",
    "- Sam: 7/10\n",
    "- Fei: 10/10\n",
    "\n",
    "**Thoughts on the presentations:** The speakers each brought unique expertise; Nadia’s insights into practical NLP applications were strong, Jeff’s deep dives into architecture design were thorough, Sam’s coverage of emerging research felt slightly rushed, and Fei’s synthesis of theory and practice was exemplary.\n",
    "\n",
    "**More guest speakers?** Yes—additional voices from industry practitioners (e.g., AI ethics, deployment engineers) would add valuable perspective.\n",
    "\n",
    "**Final project vs. traditional exam:**\n",
    "- **Pros:** Encourages hands‑on learning, reflects real-world workflows, fosters creativity.\n",
    "- **Cons:** Harder to standardize grading, project scope can vary, may disadvantage students less familiar with tooling.\n",
    "\n",
    "**Recommendation:** Continue the final‑project format, with clear rubrics to ensure consistency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
