{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813e6c3-3f96-4a76-83d4-8a440741a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]: Code cell\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eee3a5-648a-4314-a600-04b357f46426",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Comparative Analysis of LeNet, AlexNet, VGGNet, and GoogleNet\n",
    "\n",
    "## 1. Introduction\n",
    "Convolutional Neural Networks (CNNs) have dramatically advanced the field of computer vision.\n",
    "This report explores four seminal CNN architectures — **LeNet (1998)**, **AlexNet (2012)**,\n",
    "**VGGNet (2014)**, and **GoogleNet (2014)** — highlighting their architecture, innovations, and\n",
    "impact on deep learning. These networks paved the way for modern architectures such as **ResNet**\n",
    "and **EfficientNet**.\n",
    "\n",
    "**Papers Referenced:**\n",
    "- LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). *Gradient-Based Learning Applied to Document Recognition*.\n",
    "- Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *ImageNet Classification with Deep Convolutional Neural Networks*.\n",
    "- Simonyan, K., & Zisserman, A. (2014). *Very Deep Convolutional Networks for Large-Scale Image Recognition*.\n",
    "- Szegedy, C., et al. (2014). *Going Deeper with Convolutions*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3717d8-ab16-447b-9533-ea89f4237233",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Architecture Comparison\n",
    "\n",
    "| Feature                                      | LeNet-5 (1998)          | AlexNet (2012)                   | VGGNet (2014)                    | GoogleNet (2014)                   |\n",
    "|---------------------------------------------|-------------------------|----------------------------------|----------------------------------|------------------------------------|\n",
    "| **Network Depth**                           | 7 layers               | 8 layers                         | 16–19 layers                     | 22 layers (Inception v1)           |\n",
    "| **Input Size**                              | 32×32 (grayscale)      | 224×224 (RGB)                    | 224×224 (RGB)                    | 224×224 (RGB)                      |\n",
    "| **Convolutional Layers**                    | 5×5, 6–16 filters      | 11×11, 5×5, 3×3 (96–384 filters) | 3×3 filters (64–512 filters)     | Inception modules (1×1, 3×3, 5×5)   |\n",
    "| **Activation**                              | tanh                   | ReLU                             | ReLU                             | ReLU                               |\n",
    "| **Pooling**                                 | Avg pooling            | Max pooling                      | Max pooling                      | Max pooling                        |\n",
    "| **Regularization**                          | None                   | Dropout, Data Augmentation       | Dropout                          | Dropout, Auxiliary Classifiers      |\n",
    "| **# Parameters**                            | ~60K                   | ~60 million                      | ~138 million                     | ~5 million                         |\n",
    "| **Performance (Top-5 Error on ImageNet)**   | N/A (MNIST benchmark)  | 15.3%                            | 7.3%                             | 6.7%                               |\n",
    "\n",
    "These basic differences will be elaborated in the next sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da878d-0898-4cc0-81b1-7fe17124c801",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Detailed Analysis\n",
    "\n",
    "### A. Network Depth and Complexity\n",
    "- **LeNet (1998):** Designed for digit recognition on MNIST (28×28 or 32×32). Shallow compared to modern networks.\n",
    "- **AlexNet (2012):** Deeper, large-scale network that popularized modern deep learning after winning ILSVRC 2012.\n",
    "- **VGGNet (2014):** Emphasized depth using many 3×3 layers stacked. Simple but very large in parameter count.\n",
    "- **GoogleNet (2014):** Introduced the Inception module to go ‘deeper’ in a more parameter-efficient way.\n",
    "\n",
    "### B. Convolutional Layer Design\n",
    "- **LeNet:** 5×5 filters, smaller overall.\n",
    "- **AlexNet:** Mixed 11×11, 5×5, 3×3. Emphasized GPU-based large-scale training.\n",
    "- **VGGNet:** Standardized 3×3 filters, multiple stacked layers.\n",
    "- **GoogleNet:** Inception modules (1×1, 3×3, 5×5), allowing multi-scale processing in parallel.\n",
    "\n",
    "### C. Activation Functions\n",
    "- **LeNet:** Used tanh (common in the 1990s).\n",
    "- **AlexNet:** Popularized ReLU → faster training, mitigates vanishing gradients.\n",
    "- **VGGNet, GoogleNet:** Both used ReLU as the default choice.\n",
    "\n",
    "### D. Pooling Strategy\n",
    "- **LeNet:** Average pooling (older style).\n",
    "- **AlexNet, VGGNet, GoogleNet:** Primarily max pooling, which tends to retain more salient spatial information.\n",
    "\n",
    "### E. Regularization Techniques\n",
    "- **LeNet:** No explicit regularization, small parameter count.\n",
    "- **AlexNet:** Dropout + heavy data augmentation.\n",
    "- **VGGNet:** Dropout in fully-connected layers.\n",
    "- **GoogleNet:** Dropout + **Auxiliary Classifiers** to help deeper layers train.\n",
    "\n",
    "### F. Parameter Efficiency\n",
    "- **VGGNet:** ~138M parameters, quite large.\n",
    "- **GoogleNet:** ~5M parameters (Inception design is more efficient).\n",
    "\n",
    "### G. Performance\n",
    "- **LeNet:** State-of-the-art for MNIST at the time (99%+ on digits).\n",
    "- **AlexNet:** ~15.3% top-5 error on ImageNet (2012), a breakthrough then.\n",
    "- **VGGNet:** ~7.3% top-5 error on ImageNet (2014).\n",
    "- **GoogleNet:** ~6.7% top-5 error on ImageNet, with far fewer parameters than VGGNet.\n",
    "\n",
    "### H. Key Innovations\n",
    "| Model      | Key Innovation                                                                                |\n",
    "|------------|-----------------------------------------------------------------------------------------------|\n",
    "| LeNet      | First large-scale CNN for digit recognition (backprop, convolutional layers)                  |\n",
    "| AlexNet    | Large-scale CNN on GPUs, ReLU activation, dropout                                            |\n",
    "| VGGNet     | Very deep (16–19 layers) with small filters (3×3), big improvement in performance             |\n",
    "| GoogleNet  | Inception modules for multi-scale feature extraction, very high efficiency in parameter usage |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1cf54-d745-4a7e-9ec4-c2a0ca462107",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Evolution and Impact on Modern Architectures\n",
    "\n",
    "- **ResNet (2015):** Introduced skip/residual connections to handle vanishing gradients in very deep networks, building on VGG/GoogleNet’s success.\n",
    "- **EfficientNet (2019):** Proposed compound scaling (width, depth, resolution), further improving parameter efficiency. Conceptually echoes GoogleNet’s multi-scale approach and VGG’s systematic design.\n",
    "\n",
    "**Key Takeaway**: The field shifted from *just depth* to *depth + efficiency + better regularization*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd2681-2123-4885-a223-f0ad8919ab0a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "- **LeNet** laid the foundation, proving CNNs work for digit recognition and can be trained end-to-end.\n",
    "- **AlexNet** reignited deep learning with large-scale data/GPU training, ReLUs, dropout.\n",
    "- **VGGNet** went deeper with uniform 3×3 filters, setting a standard building-block style.\n",
    "- **GoogleNet** introduced the Inception module for multi-scale processing, dramatically reducing parameter count.\n",
    "\n",
    "They paved the way for modern architectures like **ResNet** (residual learning) and **EfficientNet** (compound scaling), making CNNs the de facto method in computer vision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d28313-2f4e-480c-b5c2-e8336d6dc213",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. References\n",
    "\n",
    "1. LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). \"Gradient-Based Learning Applied to Document Recognition.\"\n",
    "2. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\"\n",
    "3. Simonyan, K., & Zisserman, A. (2014). \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\"\n",
    "4. Szegedy, C., et al. (2014). \"Going Deeper with Convolutions.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fa8f0-e150-4282-9e2a-f6b9fa64b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8]: Code cell – Bar chart comparing parameter counts\n",
    "models = [\"LeNet-5\", \"AlexNet\", \"VGGNet\", \"GoogleNet\"]\n",
    "params = [0.06, 60, 138, 5]  # approximate (millions)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(models, params)\n",
    "plt.title(\"Parameter Count (in Millions)\")\n",
    "plt.ylabel(\"Millions of Parameters\")\n",
    "plt.xlabel(\"CNN Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf840f5-5ae1-4fc9-87d1-cae5831796ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]: Code cell – Bar chart comparing ImageNet top-5 error\n",
    "models_imagenet = [\"AlexNet\", \"VGGNet\", \"GoogleNet\"]\n",
    "top5_error = [15.3, 7.3, 6.7]  # approximate\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(models_imagenet, top5_error)\n",
    "plt.title(\"Top-5 Error Rate on ImageNet (%)\")\n",
    "plt.ylabel(\"Error Rate (%)\")\n",
    "plt.xlabel(\"CNN Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6641d9d-d3e6-4716-834c-00a7447fd4af",
   "metadata": {},
   "source": [
    "# Deeper Insight into Network Innovations\n",
    "\n",
    "## Additional Discussion: Why These Designs Mattered\n",
    "\n",
    "**1. Why 1×1 Convolutions Help (as popularized by GoogleNet):**\n",
    "- A 1×1 convolution can reduce the number of feature maps in intermediate layers (sometimes called *bottleneck layers*). \n",
    "- This approach lowers the computational cost by shrinking the dimensionality before applying larger filters (3×3, 5×5). \n",
    "- For instance, if you have 256 input channels and you want to apply a 5×5 filter, you’d normally need 256×5×5 parameters per output channel. By inserting a 1×1 layer first (reducing 256 down to, say, 64 channels), you drastically cut the multiplication overhead, helping to keep GoogleNet’s parameter count near 5M.\n",
    "\n",
    "**2. Inception Modules Layout:**\n",
    "- Each Inception module in GoogleNet takes the same input feature map but applies several parallel branches:\n",
    "  - A 1×1 convolution branch (sometimes just identity),\n",
    "  - A 1×1 → 3×3 branch,\n",
    "  - A 1×1 → 5×5 branch,\n",
    "  - A 3×3 max pooling → 1×1 convolution branch,\n",
    "- Then all outputs are concatenated depth-wise. This multi-scale approach allows the network to “look” at different filter sizes in parallel, capturing both fine-grained features (1×1, 3×3) and larger context (5×5).\n",
    "\n",
    "**3. Distinctions: VGG-16 vs. VGG-19**\n",
    "- VGG is known for repeating 3×3 convolution blocks. While the original paper references multiple configurations (A to E), the two commonly used are **VGG-16** (13 conv layers + 3 fully connected) and **VGG-19** (16 conv + 3 fully connected).\n",
    "- In practice, VGG-19 is just three additional convolution layers inserted at certain blocks. It has slightly better accuracy on ImageNet, but the difference is small compared to the added compute cost.\n",
    "\n",
    "**4. Why ResNet Introduced Skip Connections:**\n",
    "- As networks went deeper (e.g., beyond 20 layers), the vanishing gradient problem became more severe. Residual learning (i.e., skip connections) allowed gradients to bypass certain layers, reducing the risk of vanishing or exploding values.\n",
    "- Conceptually, a ResNet block learns the *residual* (the difference from the input), so it’s easier to train layers that only need to refine or adjust an identity mapping. This concept borrowed from prior ideas in GoogleNet about “auxiliary classifiers” and the broader push toward deeper networks.\n",
    "\n",
    "## Additional Analysis: Comparing Efficiency vs. Accuracy\n",
    "\n",
    "When we compare these networks in terms of parameter efficiency vs. accuracy:\n",
    "\n",
    "- **AlexNet**: ~60M params, top-5 ~15.3% on ImageNet.\n",
    "- **VGG (16–19)**: ~138M params, but improved top-5 (7.3%).\n",
    "- **GoogleNet**: ~5M params—drastically fewer than VGG—yet achieving ~6.7% top-5 error. \n",
    "- This reveals how **multi-scale, parallel** operations (Inception) can beat a straightforward, but large, architecture (VGGNet) in both accuracy and efficiency.\n",
    "\n",
    "By bridging these observations, we see the next wave (ResNet, EfficientNet) focusing on:\n",
    "1. **Solving gradient flow** (ResNet skip connections).\n",
    "2. **Systematic scaling** (EfficientNet’s compound scaling for depth, width, resolution).\n",
    "3. **Preserving multi-branch efficiency** (like Inception) while continuing to deepen networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a8cf1-7847-43f7-a6bd-25f296ca2a89",
   "metadata": {},
   "source": [
    "# Additional Word on Data Augmentation and Training Tricks\n",
    "\n",
    "## Training Innovations: Data Augmentation and Regularization\n",
    "\n",
    "- **AlexNet** introduced large-scale data augmentation (random crops, mirror flips, slight color jitter) which became a standard approach to reduce overfitting in image classification. This was critical because ImageNet had ~1.2M images—a lot for the time, but still not infinite.\n",
    "- **Dropout** (introduced in AlexNet, used also in VGGNet and GoogleNet) zeroes out random neuron connections during training. It forces layers not to rely too heavily on specific inputs (co-adaptation), improving generalization.\n",
    "- **Batch Normalization** (though not in the original four networks, it was quickly adopted post-2014) further stabilized training of deeper models. GoogleNet’s “auxiliary classifiers” were also an attempt to stabilize gradients in deep networks.\n",
    "\n",
    "These regularization and training strategies—together with advanced optimizers (like Adam, which came later)—helped push accuracy further while keeping overfitting in check. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2e1ee-2d39-4521-ae27-b429559b8b7d",
   "metadata": {},
   "source": [
    "# Potential Inception Diagram (ASCII)\n",
    "\n",
    "## Diagram: Simplified Inception Module (ASCII Sketch)\n",
    "\n",
    "Below is a rough ASCII sketch of a single Inception module to visualize the parallel branches:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1fb4d-2d40-4cd7-a315-635c35b40e68",
   "metadata": {},
   "source": [
    "       Next Layer Feature Maps Feature Maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefc865-3e09-42a5-96a4-b06320444851",
   "metadata": {},
   "source": [
    "\n",
    "Each path processes the same input at different receptive field scales. Then the outputs are merged. This is the key concept behind multi-scale feature extraction in GoogleNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdde20e-9f22-4c57-a26d-e2662418b879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
